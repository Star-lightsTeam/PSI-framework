{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475688b6-fb90-4360-b0be-a61d425a1c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "提取图像特征:  29%|██▉       | 641/2194 [00:24<01:05, 23.69it/s]/root/miniconda3/lib/python3.12/site-packages/PIL/Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "提取图像特征: 100%|██████████| 2194/2194 [02:52<00:00, 12.74it/s]\n",
      "提取 BERT 文本特征: 100%|██████████| 69/69 [00:03<00:00, 19.02it/s]\n",
      "提取 BERT 文本特征: 100%|██████████| 69/69 [00:03<00:00, 20.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT 文本特征提取完成。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import RobertaTokenizer, RobertaModel, ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torch.nn import ReLU, Sigmoid, LeakyReLU\n",
    "\n",
    "# 定义设备 # Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 读取CSV文件 # Read CSV file\n",
    "df = pd.read_csv(\"最终的记录.csv\", encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "# 重命名列（确保与之前一致）# Rename columns (ensure consistency with previous ones)\n",
    "df.columns = [\n",
    "    '商品名称', '一级种类', '二级种类', '图片地址', 'sku', '文本描述', '折扣率',\n",
    "    '折扣价', '价格', '星级', '销量', '收入', '评论', 'cc-1', 'cc-2',\n",
    "    'cc-3', 'DRC-1', 'DRC-2', 'DRC-3', 'RCV-1', 'RCV-2',\n",
    "    'RCV-3', 'RSV-1', 'RSV-2', 'RSV-3'\n",
    "]\n",
    "# 重置索引\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 转换数据类型 # Data type conversion\n",
    "numeric_columns = ['折扣率', '折扣价', '价格', '销量', '收入']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 重新计算收入（如果需要） # Recalculate income (if necessary)\n",
    "df['收入'] = df['折扣价'] * df['销量']\n",
    "df['收入'] = np.log10(df['收入'].replace(0, 1))  # 避免 log10(0) 错误\n",
    "df['折扣率'] = 1 - df['折扣价'] / df['价格']\n",
    "\n",
    "# 数值特征  # Numerical Features\n",
    "numerical_features = ['折扣价', '折扣率', '价格']\n",
    "\n",
    "# 标准化数值特征 # Standardized numerical features\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# 保存 Scaler 以备后续使用  # Save the Scaler for future use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# 图像特征提取  # Image feature extraction\n",
    "def vit_encoding(image_dir, num_images=2194, device='cpu'):\n",
    "    vit_list = []\n",
    "    model_name = \"/root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3\"\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "    model = ViTModel.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "    def process_image(image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            return feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        except Exception as e:\n",
    "            print(f\"无法打开图片 {image_path}: {e}\")\n",
    "            # 返回全零张量以保持尺寸一致  # Return a tensor of all zeros to maintain consistent dimensions\n",
    "            return feature_extractor(images=Image.new('RGB', (224, 224), (0, 0, 0)), return_tensors=\"pt\")\n",
    "\n",
    "    for i in tqdm(range(num_images), desc=\"提取图像特征\"):\n",
    "        image_path = os.path.join(image_dir, f\"{i}.jpg\")\n",
    "        inputs = process_image(image_path)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_token = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # [1, hidden_size]\n",
    "            vit_list.append(cls_token.squeeze())\n",
    "    \n",
    "    vit_np = np.array(vit_list)  # [num_images, hidden_size]\n",
    "    \n",
    "    # PCA降维  # PCA dimensionality reduction\n",
    "    vit_pca = PCA(n_components=50)\n",
    "    vit_features_pca = vit_pca.fit_transform(vit_np)\n",
    "    \n",
    "    # 保存PCA模型  # Save PCA model\n",
    "    joblib.dump(vit_pca, 'vit_pca.pkl')\n",
    "    \n",
    "    return vit_features_pca\n",
    "\n",
    "# 使用您的图像路径定义 # Define using your image path\n",
    "image_directory = r\"/root/img\"\n",
    "vit_features = vit_encoding(image_directory, num_images=len(df))\n",
    "df['vit_features'] = list(vit_features)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, ViTFeatureExtractor, ViTModel\n",
    "# 设置设备\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 初始化 BERT 模型和分词器 # Initialize BERT model and tokenizer\n",
    "bert_model_name = r\"/root/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "model_bert = AutoModel.from_pretrained(bert_model_name, from_tf=False)  # 根据实际情况设置 from_tf # 根据实际情况设置 from_tf # Set `from_tf` according to the actual situation\n",
    "model_bert.to(device)\n",
    "model_bert.to(device)\n",
    "model_bert.eval()  # 设置为评估模式  # Set to evaluation mode\n",
    "\n",
    "def extract_text_features(texts, model, tokenizer, device, batch_size=32, max_length=128):\n",
    "    all_features = []\n",
    "    num_samples = len(texts)\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    \n",
    "    with torch.no_grad():  # 禁用梯度计算 # Disable gradient computation\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"提取 BERT 文本特征\"):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            batch_texts = texts[start_idx:end_idx]\n",
    "            \n",
    "            # Tokenize with fixed max_length and padding to max_length\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors='pt',\n",
    "                padding='max_length',  # 固定填充到 max_length\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            \n",
    "            # 获取模型输出  # Obtain model output\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # 对最后一个隐藏状态进行平均池化，得到每个样本的固定维度特征\n",
    "            # Perform average pooling on the last hidden state to obtain fixed-dimensional features for each sample\n",
    "            encoded_text = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # [batch_size, hidden_size]\n",
    "            \n",
    "            # 添加到特征列表\n",
    "            # Add to feature list\n",
    "            all_features.append(encoded_text)\n",
    "    \n",
    "    # 将所有批次的特征堆叠起来，得到 [num_samples, hidden_size]\n",
    "    # Stack the features of all batches to obtain [num_samples, hidden_size]\n",
    "    all_features = np.vstack(all_features)\n",
    "    return all_features\n",
    "\n",
    "# 示例 DataFrame（请根据您的数据来源调整）\n",
    "# df = pd.read_csv('your_data.csv')  # 根据您的数据来源加载 DataFrame\n",
    "\n",
    "# Sample DataFrame (Please adjust according to your data source)\n",
    "# df = pd.read_csv('your_data.csv')  # Load DataFrame based on your data source\n",
    "# 确保所有文本字段为字符串类型\n",
    "# Ensure that all text fields are of string type\n",
    "text_columns = ['商品名称', '文本描述', '一级种类', '二级种类', 'cc-2']\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# 构建“其他文本”特征 # Constructing \"other text\" features\n",
    "other_texts = (\n",
    "    df['商品名称'] + \" \" +\n",
    "    df['一级种类'] + \" \" +\n",
    "    df['二级种类'] + \" \" +\n",
    "    df['cc-1']\n",
    ").tolist()\n",
    "\n",
    "# 构建“文本描述”特征  # Constructing \"Text Description\" Features\n",
    "text_descs = df['文本描述'].tolist()\n",
    "\n",
    "# 提取 BERT 的“其他文本”特征  # Extracting the \"Other Text\" features of BERT\n",
    "bert_other_text_features = extract_text_features(\n",
    "    other_texts,\n",
    "    model_bert,\n",
    "    tokenizer_bert,\n",
    "    device,\n",
    "    batch_size=32,\n",
    "    max_length=128\n",
    ")\n",
    "df['other_text_features'] = list(bert_other_text_features)\n",
    "\n",
    "# 提取 BERT 的“文本描述”特征# Extracting the \"text description\" feature of BERT\n",
    "bert_text_desc_features = extract_text_features(\n",
    "    text_descs,\n",
    "    model_bert,\n",
    "    tokenizer_bert,\n",
    "    device,\n",
    "    batch_size=32,\n",
    "    max_length=128\n",
    ")\n",
    "df['text_desc_features'] = list(bert_text_desc_features)\n",
    "\n",
    "print(\"BERT 文本特征提取完成。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28825bdf-7321-4576-a507-3e00660b8ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "网格搜索超参数:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在训练模型: {'activation_function': Sigmoid(), 'epochs': 500, 'learning_rate': 0.001}\n",
      "Seed 42 Epoch [1/500], Loss: 1.5528\n",
      "Seed 42 Epoch [100/500], Loss: 0.0342\n",
      "Seed 42 Epoch [200/500], Loss: 0.0165\n",
      "Seed 42 Epoch [300/500], Loss: 0.0090\n",
      "Seed 42 Epoch [400/500], Loss: 0.0061\n",
      "Seed 42 Epoch [500/500], Loss: 0.0052\n",
      "Seed 42 -> MAE: 595.6324, RMSE: 1239.9548, R2: -0.1923\n",
      "Seed 23 Epoch [1/500], Loss: 1.7147\n",
      "Seed 23 Epoch [100/500], Loss: 0.0327\n",
      "Seed 23 Epoch [200/500], Loss: 0.0172\n",
      "Seed 23 Epoch [300/500], Loss: 0.0095\n",
      "Seed 23 Epoch [400/500], Loss: 0.0061\n",
      "Seed 23 Epoch [500/500], Loss: 0.0050\n",
      "Seed 23 -> MAE: 334.5562, RMSE: 537.5737, R2: 0.6173\n",
      "Seed 15 Epoch [1/500], Loss: 1.8482\n",
      "Seed 15 Epoch [100/500], Loss: 0.0381\n",
      "Seed 15 Epoch [200/500], Loss: 0.0185\n",
      "Seed 15 Epoch [300/500], Loss: 0.0102\n",
      "Seed 15 Epoch [400/500], Loss: 0.0059\n",
      "Seed 15 Epoch [500/500], Loss: 0.0053\n",
      "Seed 15 -> MAE: 495.7730, RMSE: 772.4233, R2: 0.6342\n",
      "Seed 34 Epoch [1/500], Loss: 2.2519\n",
      "Seed 34 Epoch [100/500], Loss: 0.0309\n",
      "Seed 34 Epoch [200/500], Loss: 0.0154\n",
      "Seed 34 Epoch [300/500], Loss: 0.0091\n",
      "Seed 34 Epoch [400/500], Loss: 0.0058\n",
      "Seed 34 Epoch [500/500], Loss: 0.0053\n",
      "Seed 34 -> MAE: 831.7922, RMSE: 1649.9690, R2: 0.2858\n",
      "Seed 18 Epoch [1/500], Loss: 1.3876\n",
      "Seed 18 Epoch [100/500], Loss: 0.0263\n",
      "Seed 18 Epoch [200/500], Loss: 0.0142\n",
      "Seed 18 Epoch [300/500], Loss: 0.0080\n",
      "Seed 18 Epoch [400/500], Loss: 0.0061\n",
      "Seed 18 Epoch [500/500], Loss: 0.0053\n",
      "Seed 18 -> MAE: 915.2712, RMSE: 1645.7056, R2: 0.2996\n",
      "Seed 32 Epoch [1/500], Loss: 1.5340\n",
      "Seed 32 Epoch [100/500], Loss: 0.0323\n",
      "Seed 32 Epoch [200/500], Loss: 0.0150\n",
      "Seed 32 Epoch [300/500], Loss: 0.0093\n",
      "Seed 32 Epoch [400/500], Loss: 0.0063\n",
      "Seed 32 Epoch [500/500], Loss: 0.0052\n",
      "Seed 32 -> MAE: 733.1712, RMSE: 1334.0984, R2: 0.4388\n",
      "Seed 47 Epoch [1/500], Loss: 1.3402\n",
      "Seed 47 Epoch [100/500], Loss: 0.0324\n",
      "Seed 47 Epoch [200/500], Loss: 0.0157\n",
      "Seed 47 Epoch [300/500], Loss: 0.0089\n",
      "Seed 47 Epoch [400/500], Loss: 0.0057\n",
      "Seed 47 Epoch [500/500], Loss: 0.0047\n",
      "Seed 47 -> MAE: 780.5424, RMSE: 1928.7914, R2: 0.3396\n",
      "Seed 27 Epoch [1/500], Loss: 1.5023\n",
      "Seed 27 Epoch [100/500], Loss: 0.0297\n",
      "Seed 27 Epoch [200/500], Loss: 0.0152\n",
      "Seed 27 Epoch [300/500], Loss: 0.0098\n",
      "Seed 27 Epoch [400/500], Loss: 0.0068\n",
      "Seed 27 Epoch [500/500], Loss: 0.0054\n",
      "Seed 27 -> MAE: 852.7982, RMSE: 2182.6060, R2: 0.2991\n",
      "Seed 8 Epoch [1/500], Loss: 1.3046\n",
      "Seed 8 Epoch [100/500], Loss: 0.0280\n",
      "Seed 8 Epoch [200/500], Loss: 0.0159\n",
      "Seed 8 Epoch [300/500], Loss: 0.0088\n",
      "Seed 8 Epoch [400/500], Loss: 0.0060\n",
      "Seed 8 Epoch [500/500], Loss: 0.0048\n",
      "Seed 8 -> MAE: 644.6897, RMSE: 1316.0236, R2: 0.5564\n",
      "Seed 52 Epoch [1/500], Loss: 1.2898\n",
      "Seed 52 Epoch [100/500], Loss: 0.0293\n",
      "Seed 52 Epoch [200/500], Loss: 0.0152\n",
      "Seed 52 Epoch [300/500], Loss: 0.0081\n",
      "Seed 52 Epoch [400/500], Loss: 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "网格搜索超参数: 100%|██████████| 1/1 [19:49<00:00, 1189.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 52 Epoch [500/500], Loss: 0.0048\n",
      "Seed 52 -> MAE: 843.5557, RMSE: 1530.9725, R2: -0.0122\n",
      "\n",
      "=== 实验结果汇总 ===\n",
      "   seed         RMSE         MAE        R2\n",
      "0    42  1239.954834  595.632385 -0.192331\n",
      "1    23   537.573730  334.556152  0.617346\n",
      "2    15   772.423279  495.772980  0.634166\n",
      "3    34  1649.968994  831.792236  0.285757\n",
      "4    18  1645.705566  915.271240  0.299583\n",
      "5    32  1334.098389  733.171204  0.438771\n",
      "6    47  1928.791382  780.542419  0.339567\n",
      "7    27  2182.605957  852.798218  0.299138\n",
      "8     8  1316.023560  644.689697  0.556439\n",
      "9    52  1530.972534  843.555725 -0.012160\n",
      "实验结果已保存到 'experiment_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 合并所有特征 # Merge all features\n",
    "all_features = np.concatenate([\n",
    "    df[numerical_features].values,\n",
    "    np.stack(df['text_desc_features'].values),\n",
    "    np.stack(df['vit_features'].values),\n",
    "    np.stack(df['other_text_features'].values)\n",
    "], axis=1)\n",
    "\n",
    "# 定义目标变量 # Define target variable\n",
    "targets = df['收入'].to_numpy()\n",
    "\n",
    "# 定义评估指标函数 # Define evaluation metric function\n",
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(true, predicted)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# 设置随机种子列表  # Set random seed list\n",
    "SEEDS = [42, 23, 15, 34, 18, 32, 47, 27, 8, 52]\n",
    "\n",
    "# 定义 CustomDataset 类 # Define the CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float32),\n",
    "            'targets': torch.tensor(self.targets[idx], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "# 定义 ANN 回归模型  #Define ANN regression model\n",
    "class ANNRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation_function, dropout):\n",
    "        super(ANNRegressor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(activation_function)\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            previous_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 定义训练和评估函数 #Define training and evaluation functions\n",
    "def train_and_evaluate_model(learning_rate, epochs, activation_function, random_seed, hidden_dims, dropout, all_features, targets):\n",
    "    # 设置随机种子  #Set random seeds\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # 数据分割  #Data segmentation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_features, targets, test_size=0.01, random_state=random_seed)\n",
    "    \n",
    "    # 创建Dataset和DataLoader  #Create Dataset and DataLoader\n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    test_dataset = CustomDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    input_dim = all_features.shape[1]\n",
    "    output_dim = 1  # 回归任务  #Return task\n",
    "    \n",
    "    # 初始化模型 #Initialize the model\n",
    "    model = ANNRegressor(input_dim, hidden_dims, output_dim, activation_function, dropout).to(device)\n",
    "    \n",
    "    # 定义损失函数和优化器 #Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 训练模型 #Training model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            targets_batch = batch['targets'].to(device).unsqueeze(1)  # [batch_size, 1]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * features.size(0)\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "            print(f\"Seed {random_seed} Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # 测试模型 # 测试模型\n",
    "    model.eval()\n",
    "    all_true = []\n",
    "    all_predicted = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            targets_batch = batch['targets'].to(device).unsqueeze(1)\n",
    "            outputs = model(features)\n",
    "            all_true.extend(targets_batch.cpu().numpy())\n",
    "            all_predicted.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    # 转换回原始收入值  #Convert back to the original income value\n",
    "    all_true = 10 ** np.array(all_true).flatten()\n",
    "    all_predicted = 10 ** np.array(all_predicted).flatten()\n",
    "    \n",
    "    # 计算评估指标 #Calculate evaluation indicators\n",
    "    mae, rmse, r2 = evaluate_model(all_true, all_predicted)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# 网格搜索超参数 #Grid search hyperparameters\n",
    "best_params = None\n",
    "best_r2 = float('-inf')  # 使用 R2 最大化作为选择标准\n",
    "best_metrics = {}\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001],\n",
    "    'epochs': [500],\n",
    "    'activation_function': [ Sigmoid()],\n",
    "}\n",
    "results = []  # 保存所有组合和种子实验结果  #Save all combinations and seed experiment results\n",
    "\n",
    "# 固定的 hidden_dims 和 dropout\n",
    "fixed_hidden_dims = [256, 128, 64]  # 根据需求调整 #Adjust according to demand\n",
    "fixed_dropout = 0.1  # 根据需求调整 #Adjust according to demand\n",
    "# 存储实验结果  #Store experimental results\n",
    "experiment_results = {\n",
    "    'seed': [],\n",
    "    'RMSE': [],\n",
    "    'MAE': [],\n",
    "    'R2': []\n",
    "}\n",
    "\n",
    "for params in tqdm(ParameterGrid(param_grid), desc=\"网格搜索超参数\"):\n",
    "    learning_rate = params['learning_rate']\n",
    "    epochs = params['epochs']\n",
    "    activation_function = params['activation_function']\n",
    "    seed_metrics = []  # 保存当前超参数组合下所有随机种子的结果 #Save the results of all random seeds under the current hyperparameter combination\n",
    "    print(f\"正在训练模型: {params}\")\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        mae, rmse, r2 = train_and_evaluate_model(\n",
    "            learning_rate=learning_rate,\n",
    "            epochs=epochs,\n",
    "            activation_function=activation_function,\n",
    "            random_seed=seed,\n",
    "            hidden_dims=fixed_hidden_dims,\n",
    "            dropout=fixed_dropout,\n",
    "            all_features=all_features,\n",
    "            targets=targets\n",
    "        )\n",
    "        # 记录最终的训练和验证损失以及评估指标   #Record the final training and validation losses, as well as evaluation metrics\n",
    "        experiment_results['seed'].append(seed)\n",
    "        experiment_results['RMSE'].append(rmse)\n",
    "        experiment_results['MAE'].append(mae)\n",
    "        experiment_results['R2'].append(r2)\n",
    "        \n",
    "        results.append({'seed': seed, 'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "        print(f\"Seed {seed} -> MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "# 将实验结果转换为DataFrame  #Convert the experimental results into a DataFrame\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "print(\"\\n=== 实验结果汇总 ===\")\n",
    "print(results_df)\n",
    "\n",
    "# 可选：保存实验结果到CSV文件   #Optional: Save experimental results to CSV file\n",
    "results_df.to_csv(\"ANN_best_results.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"实验结果已保存到 'experiment_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c81c8-9ca7-4085-9db4-c4ff4d41a4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
